{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***NLTK نصب و راه اندازی***\n",
        "*   یک کتابخانه پایتون است که برای تحلیل و پردازش زبان انسان به کار میرود. هدفش این است که کار با زبان طبیعی را برای پروهشگران، دانشجویان و توسعه دهندگان ساده کند.\n",
        "\n"
      ],
      "metadata": {
        "id": "imLdWXnmMCpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "نصب و راه اندازی"
      ],
      "metadata": {
        "id": "etwd_Ws8NTno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXmoMVIZJW19",
        "outputId": "93097eab-a1dc-4bd4-ce0f-73b1bc7ac704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk #نصب کتابخانه"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # فراخوانی کتابخانه\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2G-_9EKVLG2N",
        "outputId": "bb986864-0276-46bb-f319-46ac36c81ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordnet\n",
        "* با استفاده از این ماژول از کتابخانه، میتوانیم مجموعه های مترادف یا معناهای  مختلف یک کلمه را ببینیم، البته در زبان انگلیسی.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXQrzCs-OxRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn # فراخوانی wordnet از کتابخانه nltk\n",
        "synsets = wn.synsets('great')\n",
        "print(\"WordNet Synsets for computer : \", synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "utfFP6O-O2Pi",
        "outputId": "7074839a-b2b1-48f2-e07a-04c0311336f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Synsets for computer :  [Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('great.n.01').definition() # با کمک تابع definition میتوانید معانی کلماتی که از خروجی بالا بدست آوردید رو ببینید."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "collapsed": true,
        "id": "yJOZmg2bQPwf",
        "outputId": "cf2aea09-9811-46e0-f114-94535438a98f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a person who has achieved distinction and honor in some field'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gutenberg\n",
        "\n",
        "* این ماژول شامل چندین کتاب کلاسیک انگلیسی است، که میتوانید برای تمرین از آن استفاده کنید.\n",
        "\n"
      ],
      "metadata": {
        "id": "YRXNI8PYQvBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "y8Bgu1QORcQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gutenberg Files:\", gutenberg.fileids()) # دیدن تمام فایل های موجود در این ماژول"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c1zx19qGRhOy",
        "outputId": "b276cfaf-0633-467a-82c3-98a6852a38ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg Files: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SPACY نصب و راه اندازی***\n",
        "* یکی از قدرتمندترین ، سریع ترین ابزارهای پردازش زبان طبیعی در پایتون است. بر خلاف کتابخانه هایی مثل nltk که بیشتر آموزشی هستند، spacy برای کارهای عملی و تولیدی ساخته شده است."
      ],
      "metadata": {
        "id": "w52UFWdlR2_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "نصب و راه اندازی"
      ],
      "metadata": {
        "id": "tvEW9zUASW5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy # نصب کتابخانه"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQu--Oj5SYdk",
        "outputId": "f4a58454-9902-4c02-b0cc-3b6aebf20838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #فراخوانی کتابخانه"
      ],
      "metadata": {
        "id": "U1IjyQ-WSgkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# برای Tokenization, POS tagging, Lemmatization استفاده میشود.\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l8MzRu9GSn3J",
        "outputId": "183ec8d6-96ec-41be-c8a4-f1a145c34c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Tokenization آموزش***\n",
        "* شکستن متن (sentence / paragraph) به واحدهای کوچکتر به نام token (توکن).\n",
        "* هر token معمولاً یک کلمه، عدد، یا علامت نگارشی است.\n"
      ],
      "metadata": {
        "id": "wva0dCwFdC62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "با استفاده از کتابخانه nltk"
      ],
      "metadata": {
        "id": "fQ6AAaKLeRVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK Tokenization\n",
        "# Word Tokenization with NLTK\n",
        "long_text = 'Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference! During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)—are transforming text analysis.'\n",
        "\n",
        "words = nltk.word_tokenize(long_text) # جدا کردن کلمه به کلمه\n",
        "print(\"Word Tokens:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJVdYs0ZdRSe",
        "outputId": "9ea35f10-9ddb-44a9-9d9a-608d91c5f2dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Dr.', 'Smith', ',', 'traveled', 'to', 'Washington', ',', 'D.C.', 'on', 'Jan.', '5th', 'for', 'a', 'cutting-edge', 'NLP', 'conference', '!', 'During', 'his', 'keynote', ',', 'he', 'explained', 'that', 'advancements', 'in', 'tokenization', 'techniques—particularly', 'those', 'implemented', 'in', 'NLTK', 'and', 'spaCy', '(', 'e.g.', ',', 'handling', 'abbreviations', 'like', '``', 'Dr.', \"''\", 'and', '``', 'e.g', '.', \"''\", 'seamlessly', ')', '—are', 'transforming', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization with NLTK:\n",
        "sentences = nltk.sent_tokenize(long_text)\n",
        "print(\"Sentence Tokens:\", sentences)\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx3rKF8OeJ3x",
        "outputId": "30d9c1b6-48df-4fa6-fe1d-dd58afc2c11d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference!', 'During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\"', 'seamlessly)—are transforming text analysis.']\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "با استفاده از کتابخانه spacy"
      ],
      "metadata": {
        "id": "447_KSrRef2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy Tokenization\n",
        "# Word Tokenization with spaCy:\n",
        "\n",
        "doc = nlp(long_text)\n",
        "words = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VQrzNQvoejHV",
        "outputId": "32cd302f-5a9e-409e-9c15-97a12ec47b85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Dr.', 'Smith', ',', 'traveled', 'to', 'Washington', ',', 'D.C.', 'on', 'Jan.', '5th', 'for', 'a', 'cutting', '-', 'edge', 'NLP', 'conference', '!', 'During', 'his', 'keynote', ',', 'he', 'explained', 'that', 'advancements', 'in', 'tokenization', 'techniques', '—', 'particularly', 'those', 'implemented', 'in', 'NLTK', 'and', 'spaCy', '(', 'e.g.', ',', 'handling', 'abbreviations', 'like', '\"', 'Dr.', '\"', 'and', '\"', 'e.g.', '\"', 'seamlessly)—are', 'transforming', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization with spaCy\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentences)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBk7Ov-sfH6J",
        "outputId": "fcea3f04-be7b-4f4c-b6d8-49344069d473"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference!', 'During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)—are transforming text analysis.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Regular Expression آموزش***\n",
        "* الگوی جستجوی متنی برای پیدا کردن، بررسی یا جایگزینی رشته‌ها در متن.\n",
        "به زبان ساده: یک قالب برای مطابقت با متن."
      ],
      "metadata": {
        "id": "PUookr9Uf6Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "با استفاده از کتابخانه nltk"
      ],
      "metadata": {
        "id": "238_5fYggK5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer # فراخوانی کتابخانه"
      ],
      "metadata": {
        "id": "ryLxrdDEgM5d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world! This is an example: email@example.com, phone: 123-456-7890.\"\n",
        "tokenizer = RegexpTokenizer(r'\\w') # ==> read word by word | کلمه به کلمه بخوان.\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MFJnkX75gQIP",
        "outputId": "f16d8ca6-a18d-4345-bbdb-8661933399d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', 'T', 'h', 'i', 's', 'i', 's', 'a', 'n', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 'e', 'm', 'a', 'i', 'l', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 'c', 'o', 'm', 'p', 'h', 'o', 'n', 'e', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "با استفاده از کتابخانه spacy"
      ],
      "metadata": {
        "id": "A6lmGgAchH6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher # برای جستجوی الگو در متن استفاده میشود"
      ],
      "metadata": {
        "id": "qCjYnPDChKqG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\") # load the small English model\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "iIpR7soEhSTE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#تعریف الگو\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}]\n",
        "matcher.add(\"CAPITAL_PATTERN\", [pattern])"
      ],
      "metadata": {
        "id": "RC2i4ARliSIT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world! This is an Example sentence.\"\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "zGR_PRnxi1mO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(\"Matched token:\", span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fY_5G1qgi2as",
        "outputId": "78689f22-1afe-47bf-d8de-d46c83bb8ec8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched token: Hello\n",
            "Matched token: This\n",
            "Matched token: Example\n"
          ]
        }
      ]
    }
  ]
}